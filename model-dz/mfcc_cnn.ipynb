{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/cuilab/AAI2022Fall-Project/tutorial'"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchinfo import summary\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 16000\n",
    "SEED = 23333\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    GPU_ID = 0\n",
    "    DEVICE = torch.device(f\"cuda:{GPU_ID}\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_files(path):\n",
    "\n",
    "    dataset = []\n",
    "    walker = sorted(str(p) for p in Path(path).glob(f\"*.flac\"))\n",
    "\n",
    "    for i, file_path in enumerate(walker):\n",
    "        path, filename = os.path.split(file_path)\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "        dataset.append(waveform)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of spk1 dataset: 100\n",
      "Len of spk2 dataset: 102\n"
     ]
    }
   ],
   "source": [
    "data_spk1 = load_audio_files(\"../LibriSpeech-SI/train/spk001/\")\n",
    "data_spk2 = load_audio_files(\"../LibriSpeech-SI/train/spk002/\")\n",
    "\n",
    "print(\"Len of spk1 dataset:\", len(data_spk1))\n",
    "print(\"Len of spk2 dataset:\", len(data_spk2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 840, 400])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def frame(signal, frame_length, frame_step, pad_end=False, pad_value=0, axis=-1):\n",
    "    \"\"\"\n",
    "    equivalent of tf.signal.frame\n",
    "    \"\"\"\n",
    "    signal_length = signal.shape[axis]\n",
    "    if pad_end:\n",
    "        frames_overlap = frame_length - frame_step\n",
    "        rest_samples = np.abs(signal_length - frames_overlap) % np.abs(\n",
    "            frame_length - frames_overlap\n",
    "        )\n",
    "        pad_size = int(frame_length - rest_samples)\n",
    "        if pad_size != 0:\n",
    "            pad_axis = [0] * signal.ndim\n",
    "            pad_axis[axis] = pad_size\n",
    "            signal = nn.functional.pad(signal, pad_axis, \"constant\", pad_value)\n",
    "    frames = signal.unfold(axis, frame_length, frame_step)\n",
    "    return frames\n",
    "\n",
    "\n",
    "frame_time = 0.025  # ms\n",
    "offset_time = 0.01  # ms\n",
    "\n",
    "window_size = int(SAMPLE_RATE * frame_time)\n",
    "offset = int(SAMPLE_RATE * offset_time)\n",
    "\n",
    "frames = frame(data_spk1[0], window_size, offset)\n",
    "frames.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然而，MFCC 自带分帧功能，因为它本来的设计就是针对一帧做的，如果把一帧扔进 MFCC 那出来的图像 size 巨小 \n",
    "\n",
    "我们的目标不是把原音频切成一帧一帧这么细，主要是切成等长，所以这里直接利用分帧函数实现一个滑动窗口，把一段音频切成 3s 一个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 48000])"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def slide_window(signal, frame_length, frame_step, pad_end=False, pad_value=0, axis=-1):\n",
    "    signal_length = signal.shape[axis]\n",
    "\n",
    "    if pad_end:\n",
    "        frames_overlap = frame_length - frame_step\n",
    "        rest_samples = np.abs(signal_length - frames_overlap) % np.abs(\n",
    "            frame_length - frames_overlap\n",
    "        )\n",
    "        pad_size = int(frame_length - rest_samples)\n",
    "        if pad_size != 0:\n",
    "            pad_axis = [0] * signal.ndim\n",
    "            pad_axis[axis] = pad_size\n",
    "            signal = nn.functional.pad(signal, pad_axis, \"constant\", pad_value)\n",
    "\n",
    "    frames = signal.unfold(axis, frame_length, frame_step)\n",
    "    return frames\n",
    "\n",
    "\n",
    "audio_time = 3  # s\n",
    "offset_time = audio_time / 2  # s\n",
    "\n",
    "window_size = int(SAMPLE_RATE * audio_time)  # 1s 16000个采样点 所以一个window 3s 48000个点\n",
    "offset = int(SAMPLE_RATE * offset_time)\n",
    "\n",
    "sub_audios = slide_window(data_spk1[0], window_size, offset)  # 这里不用pad 不足3s的剩余部分会扔掉\n",
    "sub_audios.shape  # 1声道 4个sub-sample 长度48000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of spectrogram: torch.Size([1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "mfcc_transformer = torchaudio.transforms.MFCC(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_mfcc=64,\n",
    "    melkwargs={\n",
    "        \"n_fft\": 750,\n",
    "        \"hop_length\": 750,\n",
    "        \"n_mels\": 64,\n",
    "        \"center\": False,\n",
    "        \"normalized\": True,\n",
    "    },\n",
    ")\n",
    "\n",
    "mfcc_spectrogram = mfcc_transformer(sub_audios[:, 0, :])\n",
    "print(\"Shape of spectrogram: {}\".format(mfcc_spectrogram.size()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H=n_mfcc W=(48000-n_fft)/hop_len + 1\n",
    "\n",
    "所以如果 W=64, 则 48000-n_fft 是 63 的倍数, 48000-750=63*750, 所以 n_fft=750 hop_len=750 就可以硬凑出 (64, 64) 的图片\n",
    "\n",
    "但是是为了尺寸瞎调的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.4087e+02, -4.4447e+02, -4.7305e+02,  ..., -2.4523e+02,\n",
       "          -2.4430e+02, -2.6662e+02],\n",
       "         [ 4.9197e+01,  5.1823e+01,  6.3053e+01,  ...,  6.6830e+01,\n",
       "           6.5495e+01,  7.4473e+01],\n",
       "         [-2.2102e+01, -1.9115e+01, -7.9149e+00,  ..., -6.4991e+01,\n",
       "          -6.9019e+01, -3.8841e+01],\n",
       "         ...,\n",
       "         [ 1.6700e+00,  1.0622e+00,  8.0348e-01,  ..., -6.2060e+00,\n",
       "          -6.6392e+00,  2.7292e+00],\n",
       "         [ 1.2171e+00,  5.3775e-01,  1.9720e+00,  ..., -9.9025e+00,\n",
       "          -2.5611e+00,  6.2259e+00],\n",
       "         [ 1.2759e-01, -1.0304e+00,  2.1441e-01,  ..., -1.7091e+00,\n",
       "          -6.0795e-01,  8.3641e-01]]])"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfcc_spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0879, 0.0818, 0.0329,  ..., 0.4224, 0.4240, 0.3858],\n",
       "         [0.9258, 0.9303, 0.9494,  ..., 0.9559, 0.9536, 0.9690],\n",
       "         [0.8039, 0.8090, 0.8281,  ..., 0.7305, 0.7237, 0.7752],\n",
       "         ...,\n",
       "         [0.8445, 0.8435, 0.8430,  ..., 0.8310, 0.8303, 0.8463],\n",
       "         [0.8437, 0.8426, 0.8450,  ..., 0.8247, 0.8373, 0.8523],\n",
       "         [0.8419, 0.8399, 0.8420,  ..., 0.8387, 0.8406, 0.8431]]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def minmax_scale(x):\n",
    "    return (x - x.min()) / (x.max() - x.min())\n",
    "\n",
    "\n",
    "minmax_scale(mfcc_spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1339, 1, 64, 64)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(1339,)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_time = 3  # s\n",
    "offset_time = audio_time / 2  # s\n",
    "\n",
    "window_size = int(SAMPLE_RATE * audio_time)  # 1s 16000个采样点 所以一个window 3s 48000个点\n",
    "offset = int(SAMPLE_RATE * offset_time)\n",
    "\n",
    "mfcc_transformer = torchaudio.transforms.MFCC(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_mfcc=64,\n",
    "    melkwargs={\n",
    "        \"n_fft\": 750,\n",
    "        \"hop_length\": 750,\n",
    "        \"n_mels\": 64,\n",
    "        \"center\": False,\n",
    "        \"normalized\": True,\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "def create_mfccs(data):\n",
    "    mfcc_list = []\n",
    "    for waveform in data:\n",
    "        pad = False\n",
    "        if waveform.shape[-1] < window_size:\n",
    "            pad = True\n",
    "        sub_waveforms = slide_window(\n",
    "            waveform, window_size, offset, pad_end=pad\n",
    "        ).squeeze(0)\n",
    "\n",
    "        for sub_waveform in sub_waveforms:\n",
    "            mfcc = mfcc_transformer(sub_waveform[None, :])  # (1, 64, 64)\n",
    "            mfcc_list.append(mfcc.numpy())\n",
    "\n",
    "    return mfcc_list\n",
    "\n",
    "\n",
    "xs = []\n",
    "ys = []\n",
    "for i, data in enumerate([data_spk1, data_spk2]):\n",
    "    mfccs = create_mfccs(data)\n",
    "    xs += mfccs\n",
    "    ys += [i] * len(mfccs)\n",
    "\n",
    "xs = np.array(xs)\n",
    "ys = np.array(ys)\n",
    "\n",
    "xs.shape  # (N_all, C, H, W)\n",
    "ys.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dataloaders(x, y, train_size=0.7, val_size=0.1, batch_size=32):\n",
    "    num_samples = x.shape[0]\n",
    "    split1 = int(num_samples * train_size)\n",
    "    split2 = int(num_samples * (train_size + val_size))\n",
    "\n",
    "    # shuffle\n",
    "    indices = np.random.permutation(np.arange(num_samples))\n",
    "    x = x[indices]\n",
    "    y = y[indices]\n",
    "\n",
    "    x = torch.FloatTensor(x)\n",
    "    y = torch.LongTensor(y)\n",
    "\n",
    "    x_train = x[:split1]\n",
    "    x_val = x[split1:split2]\n",
    "    x_test = x[split2:]\n",
    "\n",
    "    y_train = y[:split1]\n",
    "    y_val = y[split1:split2]\n",
    "    y_test = y[split2:]\n",
    "\n",
    "    print(f\"Trainset:\\tx-{x_train.size()}\\ty-{y_train.size()}\")\n",
    "    print(f\"Valset:  \\tx-{x_val.size()}\\ty-{y_val.size()}\")\n",
    "    print(f\"Testset:\\tx-{x_test.size()}\\ty-{y_test.size()}\")\n",
    "\n",
    "    trainset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    valset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    testset = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "\n",
    "    trainset_loader = torch.utils.data.DataLoader(\n",
    "        trainset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    valset_loader = torch.utils.data.DataLoader(\n",
    "        valset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    testset_loader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    return trainset_loader, valset_loader, testset_loader\n",
    "\n",
    "\n",
    "def onehot_decode(label):\n",
    "    return torch.argmax(label, dim=1)\n",
    "\n",
    "\n",
    "def accuracy(predictions, targets):\n",
    "    pred_decode = onehot_decode(predictions)\n",
    "    true_decode = targets\n",
    "\n",
    "    assert len(pred_decode) == len(true_decode)\n",
    "\n",
    "    acc = torch.mean((pred_decode == true_decode).float())\n",
    "\n",
    "    return float(acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "SimpleCLS                                [32, 2]                   --\n",
       "├─Sequential: 1-1                        [32, 16, 4, 4]            --\n",
       "│    └─ConvBNReLU: 2-1                   [32, 8, 32, 32]           --\n",
       "│    │    └─Conv2d: 3-1                  [32, 8, 32, 32]           80\n",
       "│    │    └─BatchNorm2d: 3-2             [32, 8, 32, 32]           16\n",
       "│    │    └─ReLU: 3-3                    [32, 8, 32, 32]           --\n",
       "│    └─MaxPool2d: 2-2                    [32, 8, 16, 16]           --\n",
       "│    └─ConvBNReLU: 2-3                   [32, 16, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-4                  [32, 16, 14, 14]          1,168\n",
       "│    │    └─BatchNorm2d: 3-5             [32, 16, 14, 14]          32\n",
       "│    │    └─ReLU: 3-6                    [32, 16, 14, 14]          --\n",
       "│    └─MaxPool2d: 2-4                    [32, 16, 7, 7]            --\n",
       "│    └─ConvBNReLU: 2-5                   [32, 16, 4, 4]            --\n",
       "│    │    └─Conv2d: 3-7                  [32, 16, 4, 4]            2,320\n",
       "│    │    └─BatchNorm2d: 3-8             [32, 16, 4, 4]            32\n",
       "│    │    └─ReLU: 3-9                    [32, 16, 4, 4]            --\n",
       "├─Sequential: 1-2                        [32, 2]                   --\n",
       "│    └─Linear: 2-6                       [32, 2]                   514\n",
       "==========================================================================================\n",
       "Total params: 4,162\n",
       "Trainable params: 4,162\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 11.15\n",
       "==========================================================================================\n",
       "Input size (MB): 0.52\n",
       "Forward/backward pass size (MB): 5.93\n",
       "Params size (MB): 0.02\n",
       "Estimated Total Size (MB): 6.47\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 128,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "class ConvBNReLU(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, padding=0):\n",
    "        super(ConvBNReLU, self).__init__()\n",
    "\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.padding = padding\n",
    "\n",
    "        self.conv = nn.Conv2d(\n",
    "            in_channels=self.in_channels,\n",
    "            out_channels=self.out_channels,\n",
    "            kernel_size=self.kernel_size,\n",
    "            stride=self.stride,\n",
    "            padding=self.padding,\n",
    "            bias=True,\n",
    "        )\n",
    "        self.bn = nn.BatchNorm2d(num_features=self.out_channels)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv(x)\n",
    "        out = self.bn(out)\n",
    "        out = self.relu(out)\n",
    "        return out\n",
    "\n",
    "    def combine_conv_bn(self):\n",
    "        conv_result = nn.Conv2d(\n",
    "            self.in_channels,\n",
    "            self.out_channels,\n",
    "            self.kernel_size,\n",
    "            stride=self.stride,\n",
    "            padding=self.padding,\n",
    "            bias=True,\n",
    "        )\n",
    "\n",
    "        scales = self.bn.weight / torch.sqrt(self.bn.running_var + self.bn.eps)\n",
    "        conv_result.bias[:] = (\n",
    "            self.conv.bias - self.bn.running_mean\n",
    "        ) * scales + self.bn.bias\n",
    "        for ch in range(self.out_channels):\n",
    "            conv_result.weight[ch, :, :, :] = self.conv.weight[ch, :, :, :] * scales[ch]\n",
    "\n",
    "        return conv_result\n",
    "\n",
    "\n",
    "class SimpleCLS(nn.Module):\n",
    "    def __init__(self, input_size=64, num_cls=2):\n",
    "        super(SimpleCLS, self).__init__()\n",
    "\n",
    "        self.input_size = input_size\n",
    "\n",
    "        self.backbone = nn.Sequential(\n",
    "            ConvBNReLU(1, 8, 3, 2, 1),  # 64 -> 32\n",
    "            nn.MaxPool2d(2, 2),  # 32 -> 16\n",
    "            ConvBNReLU(8, 16, 3, 1),  # 16 -> 14\n",
    "            nn.MaxPool2d(2, 2),  # 14 -> 7\n",
    "            ConvBNReLU(16, 16, 3, 2, 1),  # 7 -> 4\n",
    "        )\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(in_features=16 * 4 * 4, out_features=num_cls, bias=True)\n",
    "        )\n",
    "\n",
    "        self.softmax = nn.Softmax(dim=-1)\n",
    "\n",
    "        self.set_params()\n",
    "        self.train_phase()\n",
    "\n",
    "    def set_params(self):\n",
    "        for m in self.backbone.children():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.xavier_normal_(m.weight.data)\n",
    "                m.bias.data.fill_(0.02)\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "        for m in self.classifier.children():\n",
    "            if isinstance(m, nn.Linear):\n",
    "                nn.init.xavier_uniform_(m.weight)\n",
    "            elif isinstance(m, nn.BatchNorm1d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def train_phase(self):\n",
    "        self.phase = \"train\"\n",
    "\n",
    "    def test_phase(self):\n",
    "        self.phase = \"test\"\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.backbone(x)\n",
    "        # out = self.classifier(out.view(x.size(0), -1))\n",
    "        out = torch.flatten(out, 1)\n",
    "        out = self.classifier(out)\n",
    "\n",
    "        return self.softmax(out) if self.phase == \"test\" else out\n",
    "\n",
    "\n",
    "model = SimpleCLS()\n",
    "summary(model, [32, 1, 64, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def eval_model(model, valset_loader, criterion):\n",
    "    model.eval()\n",
    "    batch_loss_list = []\n",
    "    batch_acc_list = []\n",
    "    for x_batch, y_batch in valset_loader:\n",
    "        x_batch = x_batch.to(DEVICE)\n",
    "        y_batch = y_batch.to(DEVICE)\n",
    "\n",
    "        out_batch = model.forward(x_batch)\n",
    "        loss = criterion.forward(out_batch, y_batch)\n",
    "        batch_loss_list.append(loss.item())\n",
    "\n",
    "        acc = accuracy(out_batch, y_batch)\n",
    "        batch_acc_list.append(acc)\n",
    "\n",
    "    return np.mean(batch_loss_list), np.mean(batch_acc_list)\n",
    "\n",
    "\n",
    "def train_one_epoch(model, trainset_loader, optimizer, criterion):\n",
    "    model.train()\n",
    "    batch_loss_list = []\n",
    "    batch_acc_list = []\n",
    "    for x_batch, y_batch in trainset_loader:\n",
    "        x_batch = x_batch.to(DEVICE)\n",
    "        y_batch = y_batch.to(DEVICE)\n",
    "\n",
    "        out_batch = model.forward(x_batch)\n",
    "        loss = criterion.forward(out_batch, y_batch)\n",
    "        batch_loss_list.append(loss.item())\n",
    "\n",
    "        acc = accuracy(out_batch, y_batch)\n",
    "        batch_acc_list.append(acc)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return np.mean(batch_loss_list), np.mean(batch_acc_list)\n",
    "\n",
    "\n",
    "def train(\n",
    "    model,\n",
    "    trainset_loader,\n",
    "    valset_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    max_epochs=100,\n",
    "    early_stop=10,\n",
    "    verbose=1,\n",
    "    plot=False,\n",
    "    log=\"train.log\",\n",
    "):\n",
    "    if log:\n",
    "        log = open(log, \"a\")\n",
    "        log.seek(0)\n",
    "        log.truncate()\n",
    "\n",
    "    wait = 0\n",
    "    min_val_loss = np.inf\n",
    "\n",
    "    train_loss_list = []\n",
    "    train_acc_list = []\n",
    "    val_loss_list = []\n",
    "    val_acc_list = []\n",
    "\n",
    "    for epoch in range(max_epochs):\n",
    "        train_loss, train_acc = train_one_epoch(\n",
    "            model, trainset_loader, optimizer, criterion\n",
    "        )\n",
    "        train_loss_list.append(train_loss)\n",
    "        train_acc_list.append(train_acc)\n",
    "\n",
    "        val_loss, val_acc = eval_model(model, valset_loader, criterion)\n",
    "        val_loss_list.append(val_loss)\n",
    "        val_acc_list.append(val_acc)\n",
    "\n",
    "        if (epoch + 1) % verbose == 0:\n",
    "            print(\n",
    "                datetime.datetime.now(),\n",
    "                \"Epoch\",\n",
    "                epoch + 1,\n",
    "                \"\\tTrain Loss = %.5f\" % train_loss,\n",
    "                \"Train acc = %.5f \" % train_acc,\n",
    "                \"Val Loss = %.5f\" % val_loss,\n",
    "                \"Val acc = %.5f \" % val_acc,\n",
    "            )\n",
    "\n",
    "            if log:\n",
    "                print(\n",
    "                    datetime.datetime.now(),\n",
    "                    \"Epoch\",\n",
    "                    epoch + 1,\n",
    "                    \"\\tTrain Loss = %.5f\" % train_loss,\n",
    "                    \"Train acc = %.5f \" % train_acc,\n",
    "                    \"Val Loss = %.5f\" % val_loss,\n",
    "                    \"Val acc = %.5f \" % val_acc,\n",
    "                    file=log,\n",
    "                )\n",
    "                log.flush()\n",
    "\n",
    "        if val_loss < min_val_loss:\n",
    "            wait = 0\n",
    "            min_val_loss = val_loss\n",
    "            best_epoch = epoch\n",
    "            best_state_dict = model.state_dict()\n",
    "        else:\n",
    "            wait += 1\n",
    "            if wait >= early_stop:\n",
    "                print(f\"Early stopping at epoch: {epoch+1}\")\n",
    "                print(f\"Best at epoch {best_epoch+1}:\")\n",
    "                print(\n",
    "                    \"Train Loss = %.5f\" % train_loss_list[best_epoch],\n",
    "                    \"Train acc = %.5f \" % train_acc_list[best_epoch],\n",
    "                )\n",
    "                print(\n",
    "                    \"Val Loss = %.5f\" % val_loss_list[best_epoch],\n",
    "                    \"Val acc = %.5f \" % val_acc_list[best_epoch],\n",
    "                )\n",
    "\n",
    "                if log:\n",
    "                    print(f\"Early stopping at epoch: {epoch+1}\", file=log)\n",
    "                    print(f\"Best at epoch {best_epoch+1}:\", file=log)\n",
    "                    print(\n",
    "                        \"Train Loss = %.5f\" % train_loss_list[best_epoch],\n",
    "                        \"Train acc = %.5f \" % train_acc_list[best_epoch],\n",
    "                        file=log,\n",
    "                    )\n",
    "                    print(\n",
    "                        \"Val Loss = %.5f\" % val_loss_list[best_epoch],\n",
    "                        \"Val acc = %.5f \" % val_acc_list[best_epoch],\n",
    "                        file=log,\n",
    "                    )\n",
    "                    log.flush()\n",
    "                break\n",
    "\n",
    "    if plot:\n",
    "        plt.plot(range(0, epoch + 1), train_loss_list, \"-\", label=\"Train Loss\")\n",
    "        plt.plot(range(0, epoch + 1), val_loss_list, \"-\", label=\"Val Loss\")\n",
    "        plt.title(\"Epoch-Loss\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Loss\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot(range(0, epoch + 1), train_acc_list, \"-\", label=\"Train Acc\")\n",
    "        plt.plot(range(0, epoch + 1), val_acc_list, \"-\", label=\"Val Acc\")\n",
    "        plt.title(\"Epoch-Accuracy\")\n",
    "        plt.xlabel(\"Epoch\")\n",
    "        plt.ylabel(\"Accuracy\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "\n",
    "    if log:\n",
    "        log.close()\n",
    "\n",
    "    # torch.save(best_state_dict, \"./saved/best_state_dict.pkl\")\n",
    "    model.load_state_dict(best_state_dict)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset:\tx-torch.Size([937, 1, 64, 64])\ty-torch.Size([937])\n",
      "Valset:  \tx-torch.Size([134, 1, 64, 64])\ty-torch.Size([134])\n",
      "Testset:\tx-torch.Size([268, 1, 64, 64])\ty-torch.Size([268])\n",
      "2022-12-24 20:07:36.672369 Epoch 1 \tTrain Loss = 0.44425 Train acc = 0.79375  Val Loss = 0.29630 Val acc = 0.86458 \n",
      "2022-12-24 20:07:36.836856 Epoch 2 \tTrain Loss = 0.11316 Train acc = 0.97396  Val Loss = 0.08759 Val acc = 0.96250 \n",
      "2022-12-24 20:07:37.002215 Epoch 3 \tTrain Loss = 0.04687 Train acc = 0.99687  Val Loss = 0.04430 Val acc = 0.98750 \n",
      "2022-12-24 20:07:37.159402 Epoch 4 \tTrain Loss = 0.02748 Train acc = 1.00000  Val Loss = 0.04934 Val acc = 0.97500 \n",
      "2022-12-24 20:07:37.275428 Epoch 5 \tTrain Loss = 0.01894 Train acc = 1.00000  Val Loss = 0.02194 Val acc = 1.00000 \n",
      "2022-12-24 20:07:37.393836 Epoch 6 \tTrain Loss = 0.01346 Train acc = 0.99792  Val Loss = 0.01981 Val acc = 1.00000 \n",
      "2022-12-24 20:07:37.516273 Epoch 7 \tTrain Loss = 0.01268 Train acc = 1.00000  Val Loss = 0.01529 Val acc = 1.00000 \n",
      "2022-12-24 20:07:37.639400 Epoch 8 \tTrain Loss = 0.00738 Train acc = 1.00000  Val Loss = 0.04819 Val acc = 0.97500 \n",
      "2022-12-24 20:07:37.753760 Epoch 9 \tTrain Loss = 0.00597 Train acc = 1.00000  Val Loss = 0.01540 Val acc = 1.00000 \n",
      "2022-12-24 20:07:37.869980 Epoch 10 \tTrain Loss = 0.00381 Train acc = 1.00000  Val Loss = 0.00953 Val acc = 1.00000 \n",
      "2022-12-24 20:07:37.986307 Epoch 11 \tTrain Loss = 0.00345 Train acc = 1.00000  Val Loss = 0.00753 Val acc = 1.00000 \n",
      "2022-12-24 20:07:38.102905 Epoch 12 \tTrain Loss = 0.00375 Train acc = 1.00000  Val Loss = 0.00854 Val acc = 1.00000 \n",
      "2022-12-24 20:07:38.218132 Epoch 13 \tTrain Loss = 0.00164 Train acc = 1.00000  Val Loss = 0.00941 Val acc = 1.00000 \n",
      "2022-12-24 20:07:38.334128 Epoch 14 \tTrain Loss = 0.00152 Train acc = 1.00000  Val Loss = 0.00652 Val acc = 1.00000 \n",
      "2022-12-24 20:07:38.448840 Epoch 15 \tTrain Loss = 0.00127 Train acc = 1.00000  Val Loss = 0.01053 Val acc = 1.00000 \n",
      "2022-12-24 20:07:38.564888 Epoch 16 \tTrain Loss = 0.00121 Train acc = 1.00000  Val Loss = 0.00910 Val acc = 1.00000 \n",
      "2022-12-24 20:07:38.681059 Epoch 17 \tTrain Loss = 0.00119 Train acc = 1.00000  Val Loss = 0.00504 Val acc = 1.00000 \n",
      "2022-12-24 20:07:38.797143 Epoch 18 \tTrain Loss = 0.00129 Train acc = 1.00000  Val Loss = 0.00460 Val acc = 1.00000 \n",
      "2022-12-24 20:07:38.913037 Epoch 19 \tTrain Loss = 0.00115 Train acc = 1.00000  Val Loss = 0.00735 Val acc = 1.00000 \n",
      "2022-12-24 20:07:39.028729 Epoch 20 \tTrain Loss = 0.00169 Train acc = 1.00000  Val Loss = 0.00430 Val acc = 1.00000 \n",
      "2022-12-24 20:07:39.142921 Epoch 21 \tTrain Loss = 0.00070 Train acc = 1.00000  Val Loss = 0.00385 Val acc = 1.00000 \n",
      "2022-12-24 20:07:39.260072 Epoch 22 \tTrain Loss = 0.00048 Train acc = 1.00000  Val Loss = 0.00305 Val acc = 1.00000 \n",
      "2022-12-24 20:07:39.375634 Epoch 23 \tTrain Loss = 0.00052 Train acc = 1.00000  Val Loss = 0.00336 Val acc = 1.00000 \n",
      "2022-12-24 20:07:39.492958 Epoch 24 \tTrain Loss = 0.00049 Train acc = 1.00000  Val Loss = 0.00259 Val acc = 1.00000 \n",
      "2022-12-24 20:07:39.608314 Epoch 25 \tTrain Loss = 0.00045 Train acc = 1.00000  Val Loss = 0.00394 Val acc = 1.00000 \n",
      "2022-12-24 20:07:39.725068 Epoch 26 \tTrain Loss = 0.00055 Train acc = 1.00000  Val Loss = 0.00274 Val acc = 1.00000 \n",
      "2022-12-24 20:07:39.839290 Epoch 27 \tTrain Loss = 0.00046 Train acc = 1.00000  Val Loss = 0.00268 Val acc = 1.00000 \n",
      "2022-12-24 20:07:39.954537 Epoch 28 \tTrain Loss = 0.00038 Train acc = 1.00000  Val Loss = 0.00265 Val acc = 1.00000 \n",
      "2022-12-24 20:07:40.067806 Epoch 29 \tTrain Loss = 0.00051 Train acc = 1.00000  Val Loss = 0.00482 Val acc = 1.00000 \n",
      "2022-12-24 20:07:40.183076 Epoch 30 \tTrain Loss = 0.00035 Train acc = 1.00000  Val Loss = 0.00210 Val acc = 1.00000 \n",
      "2022-12-24 20:07:40.299426 Epoch 31 \tTrain Loss = 0.00022 Train acc = 1.00000  Val Loss = 0.00222 Val acc = 1.00000 \n",
      "2022-12-24 20:07:40.417356 Epoch 32 \tTrain Loss = 0.00023 Train acc = 1.00000  Val Loss = 0.00286 Val acc = 1.00000 \n",
      "2022-12-24 20:07:40.533882 Epoch 33 \tTrain Loss = 0.00030 Train acc = 1.00000  Val Loss = 0.00300 Val acc = 1.00000 \n",
      "2022-12-24 20:07:40.649306 Epoch 34 \tTrain Loss = 0.00022 Train acc = 1.00000  Val Loss = 0.00430 Val acc = 1.00000 \n",
      "2022-12-24 20:07:40.764525 Epoch 35 \tTrain Loss = 0.00021 Train acc = 1.00000  Val Loss = 0.00256 Val acc = 1.00000 \n",
      "2022-12-24 20:07:40.881368 Epoch 36 \tTrain Loss = 0.00019 Train acc = 1.00000  Val Loss = 0.00168 Val acc = 1.00000 \n",
      "2022-12-24 20:07:40.996648 Epoch 37 \tTrain Loss = 0.00016 Train acc = 1.00000  Val Loss = 0.00404 Val acc = 1.00000 \n",
      "2022-12-24 20:07:41.116075 Epoch 38 \tTrain Loss = 0.00017 Train acc = 1.00000  Val Loss = 0.00333 Val acc = 1.00000 \n",
      "2022-12-24 20:07:41.232389 Epoch 39 \tTrain Loss = 0.00020 Train acc = 1.00000  Val Loss = 0.00156 Val acc = 1.00000 \n",
      "2022-12-24 20:07:41.347900 Epoch 40 \tTrain Loss = 0.00018 Train acc = 1.00000  Val Loss = 0.00165 Val acc = 1.00000 \n",
      "2022-12-24 20:07:41.462380 Epoch 41 \tTrain Loss = 0.00055 Train acc = 1.00000  Val Loss = 0.00280 Val acc = 1.00000 \n",
      "2022-12-24 20:07:41.578584 Epoch 42 \tTrain Loss = 0.00116 Train acc = 1.00000  Val Loss = 0.01031 Val acc = 1.00000 \n",
      "2022-12-24 20:07:41.693417 Epoch 43 \tTrain Loss = 0.00018 Train acc = 1.00000  Val Loss = 0.00194 Val acc = 1.00000 \n",
      "2022-12-24 20:07:41.809380 Epoch 44 \tTrain Loss = 0.00015 Train acc = 1.00000  Val Loss = 0.00243 Val acc = 1.00000 \n",
      "2022-12-24 20:07:41.924502 Epoch 45 \tTrain Loss = 0.00015 Train acc = 1.00000  Val Loss = 0.00142 Val acc = 1.00000 \n",
      "2022-12-24 20:07:42.041073 Epoch 46 \tTrain Loss = 0.00022 Train acc = 1.00000  Val Loss = 0.00196 Val acc = 1.00000 \n",
      "2022-12-24 20:07:42.159450 Epoch 47 \tTrain Loss = 0.00011 Train acc = 1.00000  Val Loss = 0.00135 Val acc = 1.00000 \n",
      "2022-12-24 20:07:42.274751 Epoch 48 \tTrain Loss = 0.00011 Train acc = 1.00000  Val Loss = 0.00142 Val acc = 1.00000 \n",
      "2022-12-24 20:07:42.390053 Epoch 49 \tTrain Loss = 0.00008 Train acc = 1.00000  Val Loss = 0.00123 Val acc = 1.00000 \n",
      "2022-12-24 20:07:42.505922 Epoch 50 \tTrain Loss = 0.00010 Train acc = 1.00000  Val Loss = 0.00206 Val acc = 1.00000 \n",
      "2022-12-24 20:07:42.620517 Epoch 51 \tTrain Loss = 0.00008 Train acc = 1.00000  Val Loss = 0.00119 Val acc = 1.00000 \n",
      "2022-12-24 20:07:42.735549 Epoch 52 \tTrain Loss = 0.00009 Train acc = 1.00000  Val Loss = 0.00114 Val acc = 1.00000 \n",
      "2022-12-24 20:07:42.849562 Epoch 53 \tTrain Loss = 0.00006 Train acc = 1.00000  Val Loss = 0.00118 Val acc = 1.00000 \n",
      "2022-12-24 20:07:42.964039 Epoch 54 \tTrain Loss = 0.00007 Train acc = 1.00000  Val Loss = 0.00130 Val acc = 1.00000 \n",
      "2022-12-24 20:07:43.079358 Epoch 55 \tTrain Loss = 0.00006 Train acc = 1.00000  Val Loss = 0.00162 Val acc = 1.00000 \n",
      "2022-12-24 20:07:43.195681 Epoch 56 \tTrain Loss = 0.00008 Train acc = 1.00000  Val Loss = 0.00137 Val acc = 1.00000 \n",
      "2022-12-24 20:07:43.310240 Epoch 57 \tTrain Loss = 0.00006 Train acc = 1.00000  Val Loss = 0.00178 Val acc = 1.00000 \n",
      "2022-12-24 20:07:43.426921 Epoch 58 \tTrain Loss = 0.00010 Train acc = 1.00000  Val Loss = 0.00116 Val acc = 1.00000 \n",
      "2022-12-24 20:07:43.542058 Epoch 59 \tTrain Loss = 0.00007 Train acc = 1.00000  Val Loss = 0.00105 Val acc = 1.00000 \n",
      "2022-12-24 20:07:43.657684 Epoch 60 \tTrain Loss = 0.00005 Train acc = 1.00000  Val Loss = 0.00110 Val acc = 1.00000 \n",
      "2022-12-24 20:07:43.772272 Epoch 61 \tTrain Loss = 0.00010 Train acc = 1.00000  Val Loss = 0.00191 Val acc = 1.00000 \n",
      "2022-12-24 20:07:43.888104 Epoch 62 \tTrain Loss = 0.00006 Train acc = 1.00000  Val Loss = 0.00121 Val acc = 1.00000 \n",
      "2022-12-24 20:07:44.002744 Epoch 63 \tTrain Loss = 0.00007 Train acc = 1.00000  Val Loss = 0.00107 Val acc = 1.00000 \n",
      "2022-12-24 20:07:44.120048 Epoch 64 \tTrain Loss = 0.00005 Train acc = 1.00000  Val Loss = 0.00244 Val acc = 1.00000 \n",
      "2022-12-24 20:07:44.234850 Epoch 65 \tTrain Loss = 0.00005 Train acc = 1.00000  Val Loss = 0.00094 Val acc = 1.00000 \n",
      "2022-12-24 20:07:44.351072 Epoch 66 \tTrain Loss = 0.00011 Train acc = 1.00000  Val Loss = 0.00116 Val acc = 1.00000 \n",
      "2022-12-24 20:07:44.465390 Epoch 67 \tTrain Loss = 0.00004 Train acc = 1.00000  Val Loss = 0.00084 Val acc = 1.00000 \n",
      "2022-12-24 20:07:44.580170 Epoch 68 \tTrain Loss = 0.00004 Train acc = 1.00000  Val Loss = 0.00091 Val acc = 1.00000 \n",
      "2022-12-24 20:07:44.695632 Epoch 69 \tTrain Loss = 0.00005 Train acc = 1.00000  Val Loss = 0.00086 Val acc = 1.00000 \n",
      "2022-12-24 20:07:44.812693 Epoch 70 \tTrain Loss = 0.00006 Train acc = 1.00000  Val Loss = 0.00118 Val acc = 1.00000 \n",
      "2022-12-24 20:07:44.927486 Epoch 71 \tTrain Loss = 0.00005 Train acc = 1.00000  Val Loss = 0.00087 Val acc = 1.00000 \n",
      "2022-12-24 20:07:45.043596 Epoch 72 \tTrain Loss = 0.00004 Train acc = 1.00000  Val Loss = 0.00218 Val acc = 1.00000 \n",
      "2022-12-24 20:07:45.157883 Epoch 73 \tTrain Loss = 0.00004 Train acc = 1.00000  Val Loss = 0.00099 Val acc = 1.00000 \n",
      "2022-12-24 20:07:45.273629 Epoch 74 \tTrain Loss = 0.00004 Train acc = 1.00000  Val Loss = 0.00103 Val acc = 1.00000 \n",
      "2022-12-24 20:07:45.387758 Epoch 75 \tTrain Loss = 0.00007 Train acc = 1.00000  Val Loss = 0.00081 Val acc = 1.00000 \n",
      "2022-12-24 20:07:45.503642 Epoch 76 \tTrain Loss = 0.00005 Train acc = 1.00000  Val Loss = 0.00075 Val acc = 1.00000 \n",
      "2022-12-24 20:07:45.619876 Epoch 77 \tTrain Loss = 0.00003 Train acc = 1.00000  Val Loss = 0.00083 Val acc = 1.00000 \n",
      "2022-12-24 20:07:45.735304 Epoch 78 \tTrain Loss = 0.00004 Train acc = 1.00000  Val Loss = 0.00118 Val acc = 1.00000 \n",
      "2022-12-24 20:07:45.849829 Epoch 79 \tTrain Loss = 0.00003 Train acc = 1.00000  Val Loss = 0.00076 Val acc = 1.00000 \n",
      "2022-12-24 20:07:45.966640 Epoch 80 \tTrain Loss = 0.00003 Train acc = 1.00000  Val Loss = 0.00069 Val acc = 1.00000 \n",
      "2022-12-24 20:07:46.082960 Epoch 81 \tTrain Loss = 0.00005 Train acc = 1.00000  Val Loss = 0.00066 Val acc = 1.00000 \n",
      "2022-12-24 20:07:46.198270 Epoch 82 \tTrain Loss = 0.00003 Train acc = 1.00000  Val Loss = 0.00072 Val acc = 1.00000 \n",
      "2022-12-24 20:07:46.313238 Epoch 83 \tTrain Loss = 0.00003 Train acc = 1.00000  Val Loss = 0.00117 Val acc = 1.00000 \n",
      "2022-12-24 20:07:46.428342 Epoch 84 \tTrain Loss = 0.00003 Train acc = 1.00000  Val Loss = 0.00070 Val acc = 1.00000 \n",
      "2022-12-24 20:07:46.542586 Epoch 85 \tTrain Loss = 0.00002 Train acc = 1.00000  Val Loss = 0.00172 Val acc = 1.00000 \n",
      "2022-12-24 20:07:46.658452 Epoch 86 \tTrain Loss = 0.00003 Train acc = 1.00000  Val Loss = 0.00077 Val acc = 1.00000 \n",
      "2022-12-24 20:07:46.774856 Epoch 87 \tTrain Loss = 0.00003 Train acc = 1.00000  Val Loss = 0.00068 Val acc = 1.00000 \n",
      "2022-12-24 20:07:46.892452 Epoch 88 \tTrain Loss = 0.00003 Train acc = 1.00000  Val Loss = 0.00106 Val acc = 1.00000 \n",
      "2022-12-24 20:07:47.006255 Epoch 89 \tTrain Loss = 0.00003 Train acc = 1.00000  Val Loss = 0.00072 Val acc = 1.00000 \n",
      "2022-12-24 20:07:47.121847 Epoch 90 \tTrain Loss = 0.00003 Train acc = 1.00000  Val Loss = 0.00105 Val acc = 1.00000 \n",
      "2022-12-24 20:07:47.236816 Epoch 91 \tTrain Loss = 0.00004 Train acc = 1.00000  Val Loss = 0.00057 Val acc = 1.00000 \n",
      "2022-12-24 20:07:47.353453 Epoch 92 \tTrain Loss = 0.00003 Train acc = 1.00000  Val Loss = 0.00076 Val acc = 1.00000 \n",
      "2022-12-24 20:07:47.467923 Epoch 93 \tTrain Loss = 0.00002 Train acc = 1.00000  Val Loss = 0.00113 Val acc = 1.00000 \n",
      "2022-12-24 20:07:47.582781 Epoch 94 \tTrain Loss = 0.00002 Train acc = 1.00000  Val Loss = 0.00063 Val acc = 1.00000 \n",
      "2022-12-24 20:07:47.698730 Epoch 95 \tTrain Loss = 0.00002 Train acc = 1.00000  Val Loss = 0.00142 Val acc = 1.00000 \n",
      "2022-12-24 20:07:47.814335 Epoch 96 \tTrain Loss = 0.00003 Train acc = 1.00000  Val Loss = 0.00058 Val acc = 1.00000 \n",
      "2022-12-24 20:07:47.929507 Epoch 97 \tTrain Loss = 0.00002 Train acc = 1.00000  Val Loss = 0.00056 Val acc = 1.00000 \n",
      "2022-12-24 20:07:48.045223 Epoch 98 \tTrain Loss = 0.00002 Train acc = 1.00000  Val Loss = 0.00054 Val acc = 1.00000 \n",
      "2022-12-24 20:07:48.162259 Epoch 99 \tTrain Loss = 0.00002 Train acc = 1.00000  Val Loss = 0.00058 Val acc = 1.00000 \n",
      "2022-12-24 20:07:48.277489 Epoch 100 \tTrain Loss = 0.00002 Train acc = 1.00000  Val Loss = 0.00102 Val acc = 1.00000 \n",
      "Test Loss = 0.00093 Test acc = 1.00000 \n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "max_epochs = 100\n",
    "lr = 0.001\n",
    "log_file = \"temp.log\"\n",
    "\n",
    "train_loader, val_loader, test_loader = get_dataloaders(\n",
    "    xs, ys, batch_size=batch_size, train_size=0.7, val_size=0.1\n",
    ")\n",
    "\n",
    "model = SimpleCLS().to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "model = train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    max_epochs=max_epochs,\n",
    "    early_stop=10,\n",
    "    verbose=1,\n",
    "    plot=False,\n",
    "    log=log_file,\n",
    ")\n",
    "\n",
    "# model.test_phase()\n",
    "test_loss, test_acc = eval_model(model, test_loader, criterion)\n",
    "print(\"Test Loss = %.5f\" % test_loss, \"Test acc = %.5f \" % test_acc)\n",
    "with open(log_file, \"a\") as f:\n",
    "    print(\"Test Loss = %.5f\" % test_loss, \"Test acc = %.5f \" % test_acc, file=f)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "torch1.12",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13 (default, Oct 21 2022, 23:50:54) \n[GCC 11.2.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "dbb5d144c232559e4834c74f7460fdbd2d81cdf95ebb0adb0d8198111b1e41d4"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
