{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/cuilab/AAI2022Fall-Project/model-dz'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torchaudio\n",
    "import matplotlib.pyplot as plt\n",
    "import IPython.display as ipd\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torchinfo import summary\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "%pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "SAMPLE_RATE = 16000\n",
    "SEED = 23333\n",
    "\n",
    "if torch.cuda.is_available():\n",
    "    GPU_ID = 0\n",
    "    DEVICE = torch.device(f\"cuda:{GPU_ID}\")\n",
    "else:\n",
    "    DEVICE = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_audio_files(path):\n",
    "\n",
    "    dataset = []\n",
    "    walker = sorted(str(p) for p in Path(path).glob(f\"*.flac\"))\n",
    "\n",
    "    for i, file_path in enumerate(walker):\n",
    "        path, filename = os.path.split(file_path)\n",
    "        waveform, sample_rate = torchaudio.load(file_path)\n",
    "        dataset.append(waveform)\n",
    "\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Len of spk1 dataset: 100\n",
      "Len of spk2 dataset: 102\n"
     ]
    }
   ],
   "source": [
    "data_spk1 = load_audio_files(\"../LibriSpeech-SI/train/spk001/\")\n",
    "data_spk2 = load_audio_files(\"../LibriSpeech-SI/train/spk002/\")\n",
    "\n",
    "print(\"Len of spk1 dataset:\", len(data_spk1))\n",
    "print(\"Len of spk2 dataset:\", len(data_spk2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 840, 400])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def frame(signal, frame_length, frame_step, pad_end=False, pad_value=0, axis=-1):\n",
    "    \"\"\"\n",
    "    equivalent of tf.signal.frame\n",
    "    \"\"\"\n",
    "    signal_length = signal.shape[axis]\n",
    "    if pad_end:\n",
    "        frames_overlap = frame_length - frame_step\n",
    "        rest_samples = np.abs(signal_length - frames_overlap) % np.abs(\n",
    "            frame_length - frames_overlap\n",
    "        )\n",
    "        pad_size = int(frame_length - rest_samples)\n",
    "        if pad_size != 0:\n",
    "            pad_axis = [0] * signal.ndim\n",
    "            pad_axis[axis] = pad_size\n",
    "            signal = nn.functional.pad(signal, pad_axis, \"constant\", pad_value)\n",
    "    frames = signal.unfold(axis, frame_length, frame_step)\n",
    "    return frames\n",
    "\n",
    "\n",
    "frame_time = 0.025  # ms\n",
    "offset_time = 0.01  # ms\n",
    "\n",
    "window_size = int(SAMPLE_RATE * frame_time)\n",
    "offset = int(SAMPLE_RATE * offset_time)\n",
    "\n",
    "frames = frame(data_spk1[0], window_size, offset)\n",
    "frames.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然而，MFCC 自带分帧功能，因为它本来的设计就是针对一帧做的，如果把一帧扔进 MFCC 那出来的图像 size 巨小 \n",
    "\n",
    "我们的目标不是把原音频切成一帧一帧这么细，主要是切成等长，所以这里直接利用分帧函数实现一个滑动窗口，把一段音频切成 3s 一个"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1, 4, 48000])"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def slide_window(signal, frame_length, frame_step, pad_end=False, pad_value=0, axis=-1):\n",
    "    signal_length = signal.shape[axis]\n",
    "\n",
    "    if pad_end:\n",
    "        frames_overlap = frame_length - frame_step\n",
    "        rest_samples = np.abs(signal_length - frames_overlap) % np.abs(\n",
    "            frame_length - frames_overlap\n",
    "        )\n",
    "        pad_size = int(frame_length - rest_samples)\n",
    "        if pad_size != 0:\n",
    "            pad_axis = [0] * signal.ndim\n",
    "            pad_axis[axis] = pad_size\n",
    "            signal = nn.functional.pad(signal, pad_axis, \"constant\", pad_value)\n",
    "\n",
    "    frames = signal.unfold(axis, frame_length, frame_step)\n",
    "    return frames\n",
    "\n",
    "\n",
    "audio_time = 3  # s\n",
    "offset_time = audio_time / 2  # s\n",
    "\n",
    "window_size = int(SAMPLE_RATE * audio_time)  # 1s 16000个采样点 所以一个window 3s 48000个点\n",
    "offset = int(SAMPLE_RATE * offset_time)\n",
    "\n",
    "sub_audios = slide_window(data_spk1[0], window_size, offset)  # 这里不用pad 不足3s的剩余部分会扔掉\n",
    "sub_audios.shape  # 1声道 x个sub-sample 长度48000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of spectrogram: torch.Size([1, 64, 64])\n"
     ]
    }
   ],
   "source": [
    "mfcc_transformer = torchaudio.transforms.MFCC(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_mfcc=64,\n",
    "    melkwargs={\n",
    "        \"n_fft\": 750,\n",
    "        \"hop_length\": 750,\n",
    "        \"n_mels\": 64,\n",
    "        \"center\": False,\n",
    "        \"normalized\": True,\n",
    "    },\n",
    ")\n",
    "\n",
    "mfcc_spectrogram = mfcc_transformer(sub_audios[:, 0, :])\n",
    "print(\"Shape of spectrogram: {}\".format(mfcc_spectrogram.size()))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "H=n_mfcc W=(48000-n_fft)/hop_len + 1\n",
    "\n",
    "所以如果 W=64, 则 48000-n_fft 是 63 的倍数, 48000-750=63*750, 所以 n_fft=750 hop_len=750 就可以硬凑出 (64, 64) 的图片\n",
    "\n",
    "但是是为了尺寸瞎调的"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-4.4087e+02, -4.4447e+02, -4.7305e+02,  ..., -2.4523e+02,\n",
       "          -2.4430e+02, -2.6662e+02],\n",
       "         [ 4.9197e+01,  5.1823e+01,  6.3053e+01,  ...,  6.6830e+01,\n",
       "           6.5495e+01,  7.4473e+01],\n",
       "         [-2.2102e+01, -1.9115e+01, -7.9149e+00,  ..., -6.4991e+01,\n",
       "          -6.9019e+01, -3.8841e+01],\n",
       "         ...,\n",
       "         [ 1.6700e+00,  1.0622e+00,  8.0348e-01,  ..., -6.2060e+00,\n",
       "          -6.6392e+00,  2.7292e+00],\n",
       "         [ 1.2171e+00,  5.3775e-01,  1.9720e+00,  ..., -9.9025e+00,\n",
       "          -2.5611e+00,  6.2259e+00],\n",
       "         [ 1.2759e-01, -1.0304e+00,  2.1441e-01,  ..., -1.7091e+00,\n",
       "          -6.0795e-01,  8.3641e-01]]])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mfcc_spectrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.0879, 0.0818, 0.0329,  ..., 0.4224, 0.4240, 0.3858],\n",
       "         [0.9258, 0.9303, 0.9494,  ..., 0.9559, 0.9536, 0.9690],\n",
       "         [0.8039, 0.8090, 0.8281,  ..., 0.7305, 0.7237, 0.7752],\n",
       "         ...,\n",
       "         [0.8445, 0.8435, 0.8430,  ..., 0.8310, 0.8303, 0.8463],\n",
       "         [0.8437, 0.8426, 0.8450,  ..., 0.8247, 0.8373, 0.8523],\n",
       "         [0.8419, 0.8399, 0.8420,  ..., 0.8387, 0.8406, 0.8431]]])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def minmax_scale(x):\n",
    "    return (x - x.min()) / (x.max() - x.min())\n",
    "\n",
    "\n",
    "minmax_scale(mfcc_spectrogram)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(946, 1, 64, 64)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "text/plain": [
       "(946,)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "audio_time = 3  # s\n",
    "offset_time = audio_time / 2  # s\n",
    "\n",
    "window_size = int(SAMPLE_RATE * audio_time)  # 1s 16000个采样点 所以一个window 3s 48000个点\n",
    "offset = int(SAMPLE_RATE * offset_time)\n",
    "\n",
    "mfcc_transformer = torchaudio.transforms.MFCC(\n",
    "    sample_rate=SAMPLE_RATE,\n",
    "    n_mfcc=64,\n",
    "    melkwargs={\n",
    "        \"n_fft\": 750,\n",
    "        \"hop_length\": 750,\n",
    "        \"n_mels\": 64,\n",
    "        \"center\": False,\n",
    "        \"normalized\": True,\n",
    "    },\n",
    ")\n",
    "\n",
    "\n",
    "def data_split(data, train_size=0.7, val_size=0.1):\n",
    "    num_samples = len(data)\n",
    "    split1 = int(num_samples * train_size)\n",
    "    split2 = int(num_samples * (train_size + val_size))\n",
    "\n",
    "    np.random.shuffle(data)\n",
    "\n",
    "    x_train = data[:split1]\n",
    "    x_val = data[split1:split2]\n",
    "    x_test = data[split2:]\n",
    "\n",
    "    return x_train, x_val, x_test\n",
    "\n",
    "\n",
    "def create_mfccs(data):\n",
    "    mfcc_list = []\n",
    "    for waveform in data:\n",
    "        pad = False\n",
    "        if waveform.shape[-1] < window_size:\n",
    "            pad = True\n",
    "        sub_waveforms = slide_window(\n",
    "            waveform, window_size, offset, pad_end=pad\n",
    "        ).squeeze(0)\n",
    "\n",
    "        for sub_waveform in sub_waveforms:\n",
    "            mfcc = mfcc_transformer(sub_waveform[None, :])  # (1, 64, 64)\n",
    "            mfcc_list.append(mfcc.numpy())\n",
    "\n",
    "    return mfcc_list\n",
    "\n",
    "\n",
    "x_train, x_val, x_test = [], [], []\n",
    "y_train, y_val, y_test = [], [], []\n",
    "for i, data in enumerate([data_spk1, data_spk2]):\n",
    "    x_train_i, x_val_i, x_test_i = data_split(data)\n",
    "\n",
    "    mfccs_train = create_mfccs(x_train_i)\n",
    "    mfccs_val = create_mfccs(x_val_i)\n",
    "    mfccs_test = create_mfccs(x_test_i)\n",
    "\n",
    "    x_train += mfccs_train\n",
    "    x_val += mfccs_val\n",
    "    x_test += mfccs_test\n",
    "\n",
    "    y_train += [i] * len(mfccs_train)\n",
    "    y_val += [i] * len(mfccs_val)\n",
    "    y_test += [i] * len(mfccs_test)\n",
    "\n",
    "x_train = np.array(x_train)\n",
    "x_val = np.array(x_val)\n",
    "x_test = np.array(x_test)\n",
    "\n",
    "y_train = np.array(y_train)\n",
    "y_val = np.array(y_val)\n",
    "y_test = np.array(y_test)\n",
    "\n",
    "x_train.shape  # (N_train, C, H, W)\n",
    "y_train.shape"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这里之前的dataloader构建错误\n",
    "\n",
    "应该先分好train test split再进行mfcc变换\n",
    "\n",
    "这是因为一个音频通过滑动窗口会产生好几个片段，因为窗口有重叠，所以这些片段首尾是有重复的，那么进行mfcc变换之后，就会出现相邻两个图片之间的内容有重叠\n",
    "\n",
    "如果这时候再shuffle然后划分的话，那么训练和测试集中的图像就有交集了\n",
    "\n",
    "所以在滑动窗口之前就应该划分好三个集，也就是相当于先把一个用户的所有flac文件先划分好"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def shuffle_xy_totensor(x, y):\n",
    "    assert len(x) == len(y)\n",
    "\n",
    "    indices = np.random.permutation(np.arange(len(x)))\n",
    "    return torch.FloatTensor(x[indices]), torch.LongTensor(y[indices])\n",
    "\n",
    "\n",
    "def get_dataloaders(x_train, x_val, x_test, y_train, y_val, y_test, batch_size=32):\n",
    "    x_train, y_train = shuffle_xy_totensor(x_train, y_train)\n",
    "    x_val, y_val = shuffle_xy_totensor(x_val, y_val)\n",
    "    x_test, y_test = shuffle_xy_totensor(x_test, y_test)\n",
    "\n",
    "    print(f\"Trainset:\\tx-{x_train.size()}\\ty-{y_train.size()}\")\n",
    "    print(f\"Valset:  \\tx-{x_val.size()}\\ty-{y_val.size()}\")\n",
    "    print(f\"Testset:\\tx-{x_test.size()}\\ty-{y_test.size()}\")\n",
    "\n",
    "    trainset = torch.utils.data.TensorDataset(x_train, y_train)\n",
    "    valset = torch.utils.data.TensorDataset(x_val, y_val)\n",
    "    testset = torch.utils.data.TensorDataset(x_test, y_test)\n",
    "\n",
    "    trainset_loader = torch.utils.data.DataLoader(\n",
    "        trainset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    valset_loader = torch.utils.data.DataLoader(\n",
    "        valset, batch_size=batch_size, shuffle=True\n",
    "    )\n",
    "    testset_loader = torch.utils.data.DataLoader(\n",
    "        testset, batch_size=batch_size, shuffle=False\n",
    "    )\n",
    "\n",
    "    return trainset_loader, valset_loader, testset_loader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "==========================================================================================\n",
       "Layer (type:depth-idx)                   Output Shape              Param #\n",
       "==========================================================================================\n",
       "SimpleCLS                                [32, 2]                   --\n",
       "├─Sequential: 1-1                        [32, 16, 4, 4]            --\n",
       "│    └─ConvBNReLU: 2-1                   [32, 8, 32, 32]           --\n",
       "│    │    └─Conv2d: 3-1                  [32, 8, 32, 32]           80\n",
       "│    │    └─BatchNorm2d: 3-2             [32, 8, 32, 32]           16\n",
       "│    │    └─ReLU: 3-3                    [32, 8, 32, 32]           --\n",
       "│    └─MaxPool2d: 2-2                    [32, 8, 16, 16]           --\n",
       "│    └─ConvBNReLU: 2-3                   [32, 16, 14, 14]          --\n",
       "│    │    └─Conv2d: 3-4                  [32, 16, 14, 14]          1,168\n",
       "│    │    └─BatchNorm2d: 3-5             [32, 16, 14, 14]          32\n",
       "│    │    └─ReLU: 3-6                    [32, 16, 14, 14]          --\n",
       "│    └─MaxPool2d: 2-4                    [32, 16, 7, 7]            --\n",
       "│    └─ConvBNReLU: 2-5                   [32, 16, 4, 4]            --\n",
       "│    │    └─Conv2d: 3-7                  [32, 16, 4, 4]            2,320\n",
       "│    │    └─BatchNorm2d: 3-8             [32, 16, 4, 4]            32\n",
       "│    │    └─ReLU: 3-9                    [32, 16, 4, 4]            --\n",
       "├─Sequential: 1-2                        [32, 2]                   --\n",
       "│    └─Linear: 2-6                       [32, 2]                   514\n",
       "==========================================================================================\n",
       "Total params: 4,162\n",
       "Trainable params: 4,162\n",
       "Non-trainable params: 0\n",
       "Total mult-adds (M): 11.15\n",
       "==========================================================================================\n",
       "Input size (MB): 0.52\n",
       "Forward/backward pass size (MB): 5.93\n",
       "Params size (MB): 0.02\n",
       "Estimated Total Size (MB): 6.47\n",
       "=========================================================================================="
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from mfcc_cnn import onehot_decode, accuracy\n",
    "from mfcc_cnn import eval_model, train_one_epoch, train\n",
    "from mfcc_cnn import SimpleCLS\n",
    "\n",
    "model = SimpleCLS()\n",
    "summary(model, [32, 1, 64, 64])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Trainset:\tx-torch.Size([946, 1, 64, 64])\ty-torch.Size([946])\n",
      "Valset:  \tx-torch.Size([127, 1, 64, 64])\ty-torch.Size([127])\n",
      "Testset:\tx-torch.Size([266, 1, 64, 64])\ty-torch.Size([266])\n",
      "2022-12-25 10:26:07.760253 Epoch 1 \tTrain Loss = 0.48501 Train acc = 0.75729  Val Loss = 0.52299 Val acc = 0.66028 \n",
      "2022-12-25 10:26:07.860755 Epoch 2 \tTrain Loss = 0.10778 Train acc = 0.97708  Val Loss = 0.04629 Val acc = 1.00000 \n",
      "2022-12-25 10:26:07.963804 Epoch 3 \tTrain Loss = 0.03652 Train acc = 0.99687  Val Loss = 0.02384 Val acc = 1.00000 \n",
      "2022-12-25 10:26:08.063118 Epoch 4 \tTrain Loss = 0.01900 Train acc = 0.99583  Val Loss = 0.01187 Val acc = 1.00000 \n",
      "2022-12-25 10:26:08.161216 Epoch 5 \tTrain Loss = 0.01462 Train acc = 0.99896  Val Loss = 0.00900 Val acc = 1.00000 \n",
      "2022-12-25 10:26:08.261300 Epoch 6 \tTrain Loss = 0.01136 Train acc = 0.99896  Val Loss = 0.00643 Val acc = 1.00000 \n",
      "2022-12-25 10:26:08.359687 Epoch 7 \tTrain Loss = 0.00717 Train acc = 1.00000  Val Loss = 0.00666 Val acc = 1.00000 \n",
      "2022-12-25 10:26:08.458326 Epoch 8 \tTrain Loss = 0.00667 Train acc = 1.00000  Val Loss = 0.01139 Val acc = 1.00000 \n",
      "2022-12-25 10:26:08.557865 Epoch 9 \tTrain Loss = 0.00546 Train acc = 1.00000  Val Loss = 0.00457 Val acc = 1.00000 \n",
      "2022-12-25 10:26:08.656305 Epoch 10 \tTrain Loss = 0.00404 Train acc = 1.00000  Val Loss = 0.00838 Val acc = 1.00000 \n",
      "2022-12-25 10:26:08.754757 Epoch 11 \tTrain Loss = 0.00287 Train acc = 1.00000  Val Loss = 0.00514 Val acc = 1.00000 \n",
      "2022-12-25 10:26:08.854411 Epoch 12 \tTrain Loss = 0.00285 Train acc = 1.00000  Val Loss = 0.00707 Val acc = 1.00000 \n",
      "2022-12-25 10:26:08.953369 Epoch 13 \tTrain Loss = 0.00187 Train acc = 1.00000  Val Loss = 0.00272 Val acc = 1.00000 \n",
      "2022-12-25 10:26:09.051702 Epoch 14 \tTrain Loss = 0.00237 Train acc = 1.00000  Val Loss = 0.01126 Val acc = 1.00000 \n",
      "2022-12-25 10:26:09.150714 Epoch 15 \tTrain Loss = 0.00181 Train acc = 1.00000  Val Loss = 0.00289 Val acc = 1.00000 \n",
      "2022-12-25 10:26:09.277284 Epoch 16 \tTrain Loss = 0.00163 Train acc = 1.00000  Val Loss = 0.00251 Val acc = 1.00000 \n",
      "2022-12-25 10:26:09.423573 Epoch 17 \tTrain Loss = 0.00145 Train acc = 1.00000  Val Loss = 0.00382 Val acc = 1.00000 \n",
      "2022-12-25 10:26:09.578213 Epoch 18 \tTrain Loss = 0.00115 Train acc = 1.00000  Val Loss = 0.00383 Val acc = 1.00000 \n",
      "2022-12-25 10:26:09.736754 Epoch 19 \tTrain Loss = 0.00110 Train acc = 1.00000  Val Loss = 0.00554 Val acc = 1.00000 \n",
      "2022-12-25 10:26:09.892807 Epoch 20 \tTrain Loss = 0.00093 Train acc = 1.00000  Val Loss = 0.00194 Val acc = 1.00000 \n",
      "2022-12-25 10:26:10.048597 Epoch 21 \tTrain Loss = 0.00100 Train acc = 1.00000  Val Loss = 0.00286 Val acc = 1.00000 \n",
      "2022-12-25 10:26:10.204692 Epoch 22 \tTrain Loss = 0.00111 Train acc = 1.00000  Val Loss = 0.00510 Val acc = 1.00000 \n",
      "2022-12-25 10:26:10.361083 Epoch 23 \tTrain Loss = 0.00077 Train acc = 1.00000  Val Loss = 0.00363 Val acc = 1.00000 \n",
      "2022-12-25 10:26:10.516978 Epoch 24 \tTrain Loss = 0.00066 Train acc = 1.00000  Val Loss = 0.00166 Val acc = 1.00000 \n",
      "2022-12-25 10:26:10.671399 Epoch 25 \tTrain Loss = 0.00069 Train acc = 1.00000  Val Loss = 0.00269 Val acc = 1.00000 \n",
      "2022-12-25 10:26:10.810529 Epoch 26 \tTrain Loss = 0.00056 Train acc = 1.00000  Val Loss = 0.00253 Val acc = 1.00000 \n",
      "2022-12-25 10:26:10.930704 Epoch 27 \tTrain Loss = 0.00053 Train acc = 1.00000  Val Loss = 0.00311 Val acc = 1.00000 \n",
      "2022-12-25 10:26:11.040076 Epoch 28 \tTrain Loss = 0.00061 Train acc = 1.00000  Val Loss = 0.00192 Val acc = 1.00000 \n",
      "2022-12-25 10:26:11.147285 Epoch 29 \tTrain Loss = 0.00050 Train acc = 1.00000  Val Loss = 0.00389 Val acc = 1.00000 \n",
      "2022-12-25 10:26:11.250390 Epoch 30 \tTrain Loss = 0.00041 Train acc = 1.00000  Val Loss = 0.00256 Val acc = 1.00000 \n",
      "2022-12-25 10:26:11.371010 Epoch 31 \tTrain Loss = 0.00052 Train acc = 1.00000  Val Loss = 0.00143 Val acc = 1.00000 \n",
      "2022-12-25 10:26:11.510540 Epoch 32 \tTrain Loss = 0.00041 Train acc = 1.00000  Val Loss = 0.00255 Val acc = 1.00000 \n",
      "2022-12-25 10:26:11.661061 Epoch 33 \tTrain Loss = 0.00037 Train acc = 1.00000  Val Loss = 0.00218 Val acc = 1.00000 \n",
      "2022-12-25 10:26:11.814638 Epoch 34 \tTrain Loss = 0.00032 Train acc = 1.00000  Val Loss = 0.00233 Val acc = 1.00000 \n",
      "2022-12-25 10:26:11.969070 Epoch 35 \tTrain Loss = 0.00033 Train acc = 1.00000  Val Loss = 0.00291 Val acc = 1.00000 \n",
      "2022-12-25 10:26:12.121839 Epoch 36 \tTrain Loss = 0.00028 Train acc = 1.00000  Val Loss = 0.00184 Val acc = 1.00000 \n",
      "2022-12-25 10:26:12.275882 Epoch 37 \tTrain Loss = 0.00024 Train acc = 1.00000  Val Loss = 0.00202 Val acc = 1.00000 \n",
      "2022-12-25 10:26:12.427510 Epoch 38 \tTrain Loss = 0.00030 Train acc = 1.00000  Val Loss = 0.00127 Val acc = 1.00000 \n",
      "2022-12-25 10:26:12.580557 Epoch 39 \tTrain Loss = 0.00030 Train acc = 1.00000  Val Loss = 0.00274 Val acc = 1.00000 \n",
      "2022-12-25 10:26:12.732478 Epoch 40 \tTrain Loss = 0.00028 Train acc = 1.00000  Val Loss = 0.00265 Val acc = 1.00000 \n",
      "2022-12-25 10:26:12.871995 Epoch 41 \tTrain Loss = 0.00029 Train acc = 1.00000  Val Loss = 0.00185 Val acc = 1.00000 \n",
      "2022-12-25 10:26:12.991127 Epoch 42 \tTrain Loss = 0.00022 Train acc = 1.00000  Val Loss = 0.00151 Val acc = 1.00000 \n",
      "2022-12-25 10:26:13.106029 Epoch 43 \tTrain Loss = 0.00022 Train acc = 1.00000  Val Loss = 0.00238 Val acc = 1.00000 \n",
      "2022-12-25 10:26:13.216801 Epoch 44 \tTrain Loss = 0.00020 Train acc = 1.00000  Val Loss = 0.00325 Val acc = 1.00000 \n",
      "2022-12-25 10:26:13.325778 Epoch 45 \tTrain Loss = 0.00019 Train acc = 1.00000  Val Loss = 0.00187 Val acc = 1.00000 \n",
      "2022-12-25 10:26:13.431925 Epoch 46 \tTrain Loss = 0.00024 Train acc = 1.00000  Val Loss = 0.00349 Val acc = 1.00000 \n",
      "2022-12-25 10:26:13.536288 Epoch 47 \tTrain Loss = 0.00016 Train acc = 1.00000  Val Loss = 0.00140 Val acc = 1.00000 \n",
      "2022-12-25 10:26:13.635671 Epoch 48 \tTrain Loss = 0.00016 Train acc = 1.00000  Val Loss = 0.00317 Val acc = 1.00000 \n",
      "Early stopping at epoch: 48\n",
      "Best at epoch 38:\n",
      "Train Loss = 0.00030 Train acc = 1.00000 \n",
      "Val Loss = 0.00127 Val acc = 1.00000 \n",
      "Test Loss = 0.00110 Test acc = 1.00000 \n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "max_epochs = 100\n",
    "lr = 0.001\n",
    "log_file = \"temp.log\"\n",
    "\n",
    "train_loader, val_loader, test_loader = get_dataloaders(\n",
    "    x_train, x_val, x_test, y_train, y_val, y_test, batch_size=batch_size\n",
    ")\n",
    "\n",
    "model = SimpleCLS().to(DEVICE)\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "model = train(\n",
    "    model,\n",
    "    train_loader,\n",
    "    val_loader,\n",
    "    optimizer,\n",
    "    criterion,\n",
    "    max_epochs=max_epochs,\n",
    "    early_stop=10,\n",
    "    verbose=1,\n",
    "    plot=False,\n",
    "    log=log_file,\n",
    ")\n",
    "\n",
    "# model.test_phase()\n",
    "test_loss, test_acc = eval_model(model, test_loader, criterion)\n",
    "print(\"Test Loss = %.5f\" % test_loss, \"Test acc = %.5f \" % test_acc)\n",
    "with open(log_file, \"a\") as f:\n",
    "    print(\"Test Loss = %.5f\" % test_loss, \"Test acc = %.5f \" % test_acc, file=f)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 2] [3, 4] [5]\n",
      "dict_values([[1, 2], [3, 4], [5]])\n",
      "dict_items([('a', [1, 2]), ('b', [3, 4]), ('c', [5])])\n"
     ]
    }
   ],
   "source": [
    "d={\"a\": [1, 2], \"b\": [3, 4], \"c\": [5]}\n",
    "g={\"a\": [1, 8], \"b\": [0, 4], \"c\": [9]}\n",
    "\n",
    "x, y, z=d.values()\n",
    "\n",
    "print(x, y, z)\n",
    "print(d.values())\n",
    "print(d.items())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.savez_compressed(\"temp.npz\", x=d, y=g)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['x', 'y']\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1, 8]"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "npz=np.load(\"temp.npz\", allow_pickle=True)\n",
    "\n",
    "print(npz.files)\n",
    "npz[\"y\"].item()[\"a\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "4"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pickle\n",
    "pickle.DEFAULT_PROTOCOL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.save({\"x\": d, \"y\":g}, \"temp.pkl\", pickle_protocol=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'x': {'a': [1, 2], 'b': [3, 4], 'c': [5]},\n",
       " 'y': {'a': [1, 8], 'b': [0, 4], 'c': [9]}}"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pkl=torch.load(\"temp.pkl\")\n",
    "pkl"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10 (default, Nov 14 2022, 12:59:47) \n[GCC 9.4.0]"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "e7370f93d1d0cde622a1f8e1c04877d8463912d04d973331ad4851f04de6915a"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
